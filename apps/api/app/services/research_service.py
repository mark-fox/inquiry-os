from collections.abc import Mapping
from typing import Any
from uuid import UUID

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import get_settings
from app.core.llm import LLMClient, get_llm_client
from app.db.models import (
    ResearchRun,
    ResearchRunStatus,
    ResearchStep,
    ResearchStepType,
    Source
)


def _basic_plan_for_query(query: str) -> dict[str, Any]:
    """
    Very simple, rule-based 'planner' that turns a query into a few
    generic sub-questions. This is a safe fallback even if no LLM
    provider is configured.
    """
    return {
        "subquestions": [
            f"What are the key concepts and terms involved in: {query}?",
            f"What are the main options, approaches, or components related to: {query}?",
            f"What are the tradeoffs, risks, and recommendations for: {query}?",
        ],
        "notes": "Generated by basic planner v0 (rule-based).",
    }


async def _generate_planner_output(
    query: str,
    llm: LLMClient | None,
) -> dict[str, Any]:
    """
    Combine the rule-based planner output with an optional LLM-backed
    completion. If the LLM call fails for any reason, we fall back to
    only the rule-based output and record the error message.
    """
    base = _basic_plan_for_query(query)

    if llm is None:
        # No LLM available (e.g. provider not implemented yet) – just use the base plan.
        return base

    prompt = (
        "You are a research planning assistant. Given the user's research "
        "question, outline 3-5 focused subquestions or investigation steps.\n\n"
        f"Research question:\n{query}\n\n"
        "Respond in plain text, each subquestion on its own line."
    )

    try:
        completion = await llm.generate(
            prompt=prompt,
            options={"max_tokens": 512},
        )

        # Keep the existing structured subquestions, but attach the full
        # LLM completion so we can inspect it or evolve the schema later.
        return {
            **base,
            "llm_completion": completion,
        }
    except Exception as exc:  # noqa: BLE001
        # Swallow LLM errors and keep the base plan intact.
        return {
            **base,
            "llm_error": str(exc),
        }


async def _next_step_index_for_run(
    run_id: UUID,
    db: AsyncSession,
) -> int:
    """
    Compute the next step_index for a given run by looking at existing steps.

    If there are no steps yet, this returns 0.
    """
    result = await db.execute(
        select(ResearchStep.step_index).where(ResearchStep.run_id == run_id)
    )
    indices = list(result.scalars().all())
    if not indices:
        return 0
    return max(indices) + 1


async def create_research_run_with_basic_plan(
    *,
    payload: Mapping[str, Any],
    db: AsyncSession,
) -> ResearchRun:
    """
    Create a new ResearchRun and a corresponding planner ResearchStep.

    The planner currently uses a combination of:
    - a simple rule-based fallback for structured subquestions
    - an optional LLM-backed completion via the LLM client abstraction
    """
    settings = get_settings()

    query = str(payload["query"])
    title = payload.get("title")

    run = ResearchRun(
        query=query,
        title=title,
        status=ResearchRunStatus.PENDING,
        model_provider=f"{settings.llm_provider}:{settings.llm_model}",
    )

    db.add(run)
    # flush so run.id is populated before we create the step
    await db.flush()

    # Try to get an LLM client; fall back to no LLM if provider is unsupported
    llm: LLMClient | None
    try:
        llm = get_llm_client()
    except Exception:  # noqa: BLE001
        llm = None

    plan_output = await _generate_planner_output(query, llm)

    step = ResearchStep(
        run_id=run.id,
        step_index=0,
        step_type=ResearchStepType.PLANNER,
        input={"query": query},
        output=plan_output,
    )

    db.add(step)
    await db.commit()
    await db.refresh(run)

    return run


async def run_dummy_search_for_run(
    run_id: UUID,
    db: AsyncSession,
) -> ResearchRun:
    """
    Dummy 'searcher' agent that attaches a simple search step and a few
    fake sources to an existing research run.

    This does NOT perform real web search. It just creates plausible
    Source rows so we can exercise the data model and UI. Later we will
    replace this with a real web search + fetch pipeline.
    """
    # Load the run first
    run = await db.get(ResearchRun, run_id)
    if run is None:
        raise ValueError("Research run not found")

    query = run.query

    # In the future we might compute step_index dynamically from the
    # current max for this run. For now we assume planner is at 0 and
    # this is the next step.
    search_step = ResearchStep(
        run_id=run.id,
        step_index=1,
        step_type=ResearchStepType.SEARCHER,
        input={"query": query},
        output={
            "notes": "Dummy searcher v0 – no real web search performed.",
            "hint": "Later this will hit a search API and populate real sources.",
        },
    )
    db.add(search_step)

    # Create a few fake sources based on the query
    base_slug = query.lower().replace(" ", "-")[:50] or "research-topic"

    sources = [
        Source(
            run_id=run.id,
            url=f"https://example.com/articles/{base_slug}-overview",
            title="High-level overview related to your research question",
            raw_content=None,
            summary="Overview article (dummy source for dev/testing).",
            relevance_score=0.9,
            extra_metadata={"source_type": "overview", "dummy": True},
        ),
        Source(
            run_id=run.id,
            url=f"https://example.com/blog/{base_slug}-tradeoffs",
            title="Discussion of tradeoffs and practical considerations",
            raw_content=None,
            summary="Tradeoffs and pros/cons (dummy source for dev/testing).",
            relevance_score=0.8,
            extra_metadata={"source_type": "discussion", "dummy": True},
        ),
        Source(
            run_id=run.id,
            url=f"https://example.com/docs/{base_slug}-reference",
            title="Reference documentation or spec-style material",
            raw_content=None,
            summary="Reference-style material (dummy source for dev/testing).",
            relevance_score=0.75,
            extra_metadata={"source_type": "reference", "dummy": True},
        ),
    ]

    db.add_all(sources)
    await db.commit()
    await db.refresh(run)

    return run


async def run_dummy_synthesis_for_run(
    run_id: UUID,
    db: AsyncSession,
) -> ResearchRun:
    """
    Dummy 'synthesizer' agent that looks at the run's sources and creates
    a synthesizer step with a simple, human-readable answer.

    This does NOT call a real LLM yet. It just stitches together a
    placeholder answer so we can exercise the data model and UI. Later
    we can replace the answer generation with an LLM-backed call.
    """
    # Load the run
    run = await db.get(ResearchRun, run_id)
    if run is None:
        raise ValueError("Research run not found")

    # Fetch sources for this run
    result = await db.execute(select(Source).where(Source.run_id == run_id))
    sources = list(result.scalars().all())

    # Build a simple answer text using the run query and sources
    if not sources:
        answer_text = (
            "No sources are currently attached to this research run. "
            "Run the searcher agent first to collect relevant sources."
        )
    else:
        lines: list[str] = []
        lines.append(
            "This is a dummy synthesized answer based on the attached sources."
        )
        lines.append("")
        lines.append(f"Research question: {run.query}")
        lines.append("")
        lines.append("The system considered the following sources:")
        for idx, src in enumerate(sources, start=1):
            title = src.title or src.url
            lines.append(f"{idx}. {title} — {src.url}")

        lines.append("")
        lines.append(
            "A proper LLM-backed synthesizer will later read and compare these "
            "sources in detail to produce a nuanced, citation-rich answer."
        )

        answer_text = "\n".join(lines)

    step_index = await _next_step_index_for_run(run_id=run_id, db=db)

    synth_step = ResearchStep(
        run_id=run.id,
        step_index=step_index,
        step_type=ResearchStepType.SYNTHESIZER,
        input={
            "source_ids": [str(s.id) for s in sources],
        },
        output={
            "answer": answer_text,
            "notes": "Dummy synthesizer v0 – no real LLM call performed.",
            "source_count": len(sources),
        },
    )

    db.add(synth_step)
    await db.commit()
    await db.refresh(run)

    return run