from collections.abc import Mapping
from typing import Any

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import get_settings
from app.core.llm import LLMClient, get_llm_client
from app.db.models import (
    ResearchRun,
    ResearchRunStatus,
    ResearchStep,
    ResearchStepType,
)


def _basic_plan_for_query(query: str) -> dict[str, Any]:
    """
    Very simple, rule-based 'planner' that turns a query into a few
    generic sub-questions. This is a safe fallback even if no LLM
    provider is configured.
    """
    return {
        "subquestions": [
            f"What are the key concepts and terms involved in: {query}?",
            f"What are the main options, approaches, or components related to: {query}?",
            f"What are the tradeoffs, risks, and recommendations for: {query}?",
        ],
        "notes": "Generated by basic planner v0 (rule-based).",
    }


async def _generate_planner_output(
    query: str,
    llm: LLMClient | None,
) -> dict[str, Any]:
    """
    Combine the rule-based planner output with an optional LLM-backed
    completion. If the LLM call fails for any reason, we fall back to
    only the rule-based output and record the error message.
    """
    base = _basic_plan_for_query(query)

    if llm is None:
        # No LLM available (e.g. provider not implemented yet) â€“ just use the base plan.
        return base

    prompt = (
        "You are a research planning assistant. Given the user's research "
        "question, outline 3-5 focused subquestions or investigation steps.\n\n"
        f"Research question:\n{query}\n\n"
        "Respond in plain text, each subquestion on its own line."
    )

    try:
        completion = await llm.generate(
            prompt=prompt,
            options={"max_tokens": 512},
        )

        # Keep the existing structured subquestions, but attach the full
        # LLM completion so we can inspect it or evolve the schema later.
        return {
            **base,
            "llm_completion": completion,
        }
    except Exception as exc:  # noqa: BLE001
        # Swallow LLM errors and keep the base plan intact.
        return {
            **base,
            "llm_error": str(exc),
        }


async def create_research_run_with_basic_plan(
    *,
    payload: Mapping[str, Any],
    db: AsyncSession,
) -> ResearchRun:
    """
    Create a new ResearchRun and a corresponding planner ResearchStep.

    The planner currently uses a combination of:
    - a simple rule-based fallback for structured subquestions
    - an optional LLM-backed completion via the LLM client abstraction
    """
    settings = get_settings()

    query = str(payload["query"])
    title = payload.get("title")

    run = ResearchRun(
        query=query,
        title=title,
        status=ResearchRunStatus.PENDING,
        model_provider=f"{settings.llm_provider}:{settings.llm_model}",
    )

    db.add(run)
    # flush so run.id is populated before we create the step
    await db.flush()

    # Try to get an LLM client; fall back to no LLM if provider is unsupported
    llm: LLMClient | None
    try:
        llm = get_llm_client()
    except Exception:  # noqa: BLE001
        llm = None

    plan_output = await _generate_planner_output(query, llm)

    step = ResearchStep(
        run_id=run.id,
        step_index=0,
        step_type=ResearchStepType.PLANNER,
        input={"query": query},
        output=plan_output,
    )

    db.add(step)
    await db.commit()
    await db.refresh(run)

    return run
